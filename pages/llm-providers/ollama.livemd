# Local Instructor w/ Ollama

```elixir
Mix.install(
  [
    {:instructor, path: Path.expand("../../", __DIR__)},
    {:kino_shell, "~> 0.1.2"}
  ],
  config: [
    instructor: [
      adapter: Instructor.Adapters.OpenAI,
      openai: [
        api_key: "ollama",
        api_url: "http://localhost:11434"
      ]
    ]
  ]
)
```

## Setup Ollama

TODO

<!-- livebook:{"attrs":"eyJpbl9iYWNrZ3JvdW5kIjpmYWxzZSwicmVzdGFydCI6ZmFsc2UsInNvdXJjZSI6IiMgb2xsYW1hIHB1bGwgbGxhbWEzIn0","chunks":null,"kind":"Elixir.KinoShell.ShellScriptCell","livebook_object":"smart_cell"} -->

```elixir
{_, 0} = System.cmd("bash", ["-lc", "# ollama pull llama3"], into: IO.stream())
:ok
```

```elixir
defmodule President do
  use Ecto.Schema

  @primary_key false
  embedded_schema do
    field(:first_name, :string)
    field(:last_name, :string)
    field(:year_entered_office, :integer)
  end
end
```

```elixir
Instructor.chat_completion(
  mode: :json,
  model: "llama3",
  response_model: President,
  messages: [
    %{role: "user", content: "Who was the first president of the United States?"}
  ]
  # before_request: &IO.inspect/1,
  # after_response: &IO.inspect/1
)
```

```elixir
Instructor.chat_completion(
  model: "llama3",
  mode: :json,
  stream: true,
  response_model: {:array, President},
  messages: [
    %{role: "user", content: "Who are the first three presidents"}
  ]
)
|> Stream.each(fn {:ok, x} -> IO.inspect(x) end)
|> Stream.run()
```
